{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Setup autoreload\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Give yourself access to common\n",
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2490716]\n",
      " [3253148]\n",
      " [3433524]\n",
      " ...\n",
      " [5835848]\n",
      " [1666846]\n",
      " [ 142610]]\n"
     ]
    }
   ],
   "source": [
    "# Gather training and testing data and process it to be configured for time intervals\n",
    "from common import *\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "nba = get_cleaned_external_data()\n",
    "# nba = get_cleaned_baseline_data()\n",
    "# nba = add_log_y_values(nba)\n",
    "# X_train, X_test, y_train, y_test = split_data(nba, time_based_split=False)\n",
    "\n",
    "X = nba.loc[:, nba.columns != 'inflationAdjSalary']\n",
    "y = nba.loc[:, nba.columns == 'inflationAdjSalary']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "X_train = X_train.values\n",
    "X_test = X_test.values\n",
    "y_train = y_train.values\n",
    "y_test = y_test.values\n",
    "\n",
    "print(y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(nba.columns)\n",
    "# y_test = np.exp(y_test)\n",
    "# y_train = np.exp(y_train)\n",
    "# print(y_test)\n",
    "# nba = nba.drop(['inflationAdjSalary'], axis=1)\n",
    "\n",
    "def pad_y(y) :\n",
    "    # y = y[:,np.newaxis]\n",
    "    tmp = np.zeros((y.shape[0],len(nba.columns)))\n",
    "    tmp[:,len(nba.columns)-1:] = y\n",
    "    return tmp\n",
    "\n",
    "def pad_x(x) :\n",
    "    tmp = np.zeros((x.shape[0],len(nba.columns)))\n",
    "    tmp[:,:len(nba.columns)-1] = x\n",
    "    return tmp\n",
    "\n",
    "def unpad_x(x) :\n",
    "    return x[:,:len(nba.columns)-1]\n",
    "\n",
    "def unpad_y(y) :\n",
    "    return y[:,len(nba.columns)-1:]\n",
    "\n",
    "\n",
    "# Scale the stuff down\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# scaler = StandardScaler()\n",
    "# scaler.fit(nba)\n",
    "# X_train = unpad_x(scaler.transform(pad_x(X_train)))\n",
    "# X_test = unpad_x(scaler.transform(pad_x(X_test)))\n",
    "# y_train = unpad_y(scaler.transform(pad_y(y_train)))\n",
    "# y_test = unpad_y(scaler.transform(pad_y(y_test)))\n",
    "\n",
    "# print(y_train_scaled)\n",
    "# print(X_train_scaled[0,:])\n",
    "# Sanity check the scaler\n",
    "# print((nba['FG']).iloc[0])\n",
    "# print(X_train_scaled[0, X.columns.get_loc('FG')])\n",
    "# inv = scaler.inverse_transform(X_scaled)\n",
    "# print(inv[0, X.columns.get_loc('FG')])\n",
    "\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_scaled, test_size = 0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mmart\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:1623: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "c:\\Users\\mmart\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-6 {color: black;background-color: white;}#sk-container-id-6 pre{padding: 0;}#sk-container-id-6 div.sk-toggleable {background-color: white;}#sk-container-id-6 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-6 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-6 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-6 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-6 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-6 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-6 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-6 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-6 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-6 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-6 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-6 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-6 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-6 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-6 div.sk-item {position: relative;z-index: 1;}#sk-container-id-6 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-6 div.sk-item::before, #sk-container-id-6 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-6 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-6 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-6 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-6 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-6 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-6 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-6 div.sk-label-container {text-align: center;}#sk-container-id-6 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-6 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-6\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MLPRegressor(hidden_layer_sizes=(20, 10, 10), max_iter=1000)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" checked><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MLPRegressor</label><div class=\"sk-toggleable__content\"><pre>MLPRegressor(hidden_layer_sizes=(20, 10, 10), max_iter=1000)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MLPRegressor(hidden_layer_sizes=(20, 10, 10), max_iter=1000)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the model\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "import numpy as np\n",
    "\n",
    "# MLPClassifier only classifies data as integers or strings, therefore, our problem is one of regression for the neural network\n",
    "# Consequently I should use the mlp regressor\n",
    "mlp = MLPRegressor(hidden_layer_sizes=(20,10,10), solver='adam', max_iter=1000)\n",
    "mlp.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mmart\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_search.py:305: UserWarning: The total space of parameters 108 is smaller than n_iter=120. Running 108 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n",
      "c:\\Users\\mmart\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:1623: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'learning_rate_init': 0.01, 'hidden_layer_sizes': (20, 20, 30), 'alpha': 0.0001, 'activation': 'relu'}\n"
     ]
    }
   ],
   "source": [
    "# Tune the Hyerparameters\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "# Define the hyperparameters you want to tune\n",
    "params = {\n",
    "    'hidden_layer_sizes': [(10,), (20,20,30), (5,5,5,5), (20,10,5), (10,10,10), (20,10,10)],\n",
    "    'activation': ['relu', 'tanh'],\n",
    "    'alpha': [0.0001, 0.001, 0.01],\n",
    "    'learning_rate_init': [0.001, 0.01, 0.1]\n",
    "}\n",
    "\n",
    "# Use GridSearchCV to search over the hyperparameter grid\n",
    "rs = RandomizedSearchCV(mlp, param_distributions=params, n_iter=120, cv=5, n_jobs=-1, random_state=42)\n",
    "\n",
    "# Fit the GridSearchCV object on the training data\n",
    "rs.fit(X_train, y_train)\n",
    "\n",
    "# Print the best set of hyperparameters\n",
    "print(\"Best Hyperparameters:\", rs.best_params_)\n",
    "\n",
    "# Evaluate the model on the validation set using the best hyperparameters\n",
    "mlp = rs.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R_squared value:  0.3840255276664155\n",
      "RMSE:  0.8893512001208703\n",
      "MSE: 0.7909455571564323\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "predict_test = mlp.predict(X_test)\n",
    "test_set_rsquared = mlp.score(X_test, y_test)\n",
    "test_set_rmse = np.sqrt(mean_squared_error(np.log(predict_test), np.log(y_test)))\n",
    "test_set_mse = mean_squared_error(np.log(predict_test), np.log(y_test))\n",
    "print('R_squared value: ', r2_score(np.log(y_test), np.log(predict_test))) # Variances are not nicely correlated\n",
    "print('RMSE: ', test_set_rmse) # Fairly good at data prediction\n",
    "print('MSE:', test_set_mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_scaled' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[86], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# Denormalize the data\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m predict_test \u001b[39m=\u001b[39m mlp\u001b[39m.\u001b[39mpredict((X_scaled[\u001b[39m0\u001b[39m])\u001b[39m.\u001b[39mreshape(\u001b[39m1\u001b[39m,\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m))\n\u001b[0;32m      3\u001b[0m \u001b[39mprint\u001b[39m(X_train\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m])\n\u001b[0;32m      4\u001b[0m \u001b[39mprint\u001b[39m(predict_test)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_scaled' is not defined"
     ]
    }
   ],
   "source": [
    "# # Denormalize the data\n",
    "# predict_test = mlp.predict((X_scaled[0]).reshape(1,-1))\n",
    "# print(X_train.shape[1])\n",
    "# print(predict_test)\n",
    "# tmp = np.zeros((1,X_train.shape[1]))\n",
    "# tmp[:,:predict_test.shape[1]] = predict_test\n",
    "# print(tmp)\n",
    "# undone = scaler.inverse_transform(tmp)\n",
    "# print(undone.flatten().tolist()[:13])\n",
    "# print(y[base_feats].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Off by at most this much: [1162.6010485438574, 564.5294819959297, 0.9358547475622839, 36.678941065626624, 44.77237141825249, 5.681788518771645, 0.20083569593628162, 176.79757130821858, 50.42570265081218, 279.9271334943758, 140.38598344515225, 67.25290821346142, 211.85565399097388, 2.097975557900953, 17.64002385748879]\n",
      "[3485.  2832.    40.    85.    83.    45.3    6.   925.   225.  1226.\n",
      "  756.   402.   978.    90.   325. ]\n",
      "[  1.    0.   19.    1.    0.  -40.7   0.    0.    0.    0.    0.    0.\n",
      "   0.   63.  135. ]\n",
      "[[476.1579 496.692 ]\n",
      " [212.2525 222.3542]\n",
      " [  0.3576   0.3772]\n",
      " [ 13.7566  14.3724]\n",
      " [ 18.2243  18.9838]\n",
      " [  2.1591   2.2802]\n",
      " [  0.072    0.0783]\n",
      " [ 56.3487  59.6562]\n",
      " [ 18.633   19.5375]\n",
      " [ 99.8705 104.9828]\n",
      " [ 48.5911  51.2492]\n",
      " [ 21.9521  23.1666]\n",
      " [ 80.2483  84.0312]\n",
      " [  0.7928   0.831 ]\n",
      " [  7.0573   7.3689]]\n",
      "{'column_0': (475.79476645622674, 496.67383016962964), 'column_1': (212.41596990296983, 222.70172029234558), 'column_2': (0.35803130884824563, 0.3767257665685465), 'column_3': (13.760876366711393, 14.350450541466435), 'column_4': (18.25985083662169, 18.982653714292923), 'column_5': (2.16194368301781, 2.276515886020327), 'column_6': (0.07228690505247085, 0.07850881653611261), 'column_7': (56.42384431825786, 59.63269751811651), 'column_8': (18.637464708414942, 19.531274031203957), 'column_9': (100.04943013258044, 104.85566868220226), 'column_10': (48.65462707647754, 51.32500366985124), 'column_11': (21.997164469282435, 23.158852957745914), 'column_12': (80.12407636296908, 84.01731300550834), 'column_13': (0.7917827239438554, 0.8306204099297747), 'column_14': (7.057568917333044, 7.352953130176186)}\n",
      "HELLO\n",
      "[1017.7252  451.7522    0.6491   36.6789   35.6219    4.6071    0.1387\n",
      "  121.1894   36.891   215.9959   96.3966   42.129   167.0021    1.6828\n",
      "   13.9798]\n"
     ]
    }
   ],
   "source": [
    "# # Compute the confidence interval of each of the features\n",
    "# from common import *\n",
    "\n",
    "# # Get the unscaled predictions\n",
    "# predict_full = mlp.predict(X_scaled)\n",
    "# tmp = np.zeros(X_scaled.shape)\n",
    "# tmp[:,:predict_full.shape[1]] = predict_full\n",
    "# predicted_unscaled_vals = scaler.inverse_transform(tmp)\n",
    "# predicted_unscaled_vals = predicted_unscaled_vals[:,:predict_full.shape[1]]\n",
    "\n",
    "# # compute the RMSE for each feature in the output vector\n",
    "# actual_unscaled_vals = y[base_feats].values\n",
    "# rmse = np.sqrt(np.square(predicted_unscaled_vals - actual_unscaled_vals))\n",
    "\n",
    "# # compute the percentilse of the RMSE to get the confidence interval of each predicted feature\n",
    "# ci_up = np.percentile(rmse, 95, axis=0) # 95th percentile\n",
    "# ci_low = np.percentile(rmse, 5, axis=0) # 5th percentile\n",
    "# print(\"Off by at most this much:\", ci_up.flatten().tolist())\n",
    "# print(np.max(actual_unscaled_vals, axis=0))\n",
    "# print(np.min(actual_unscaled_vals, axis=0)) \n",
    "# # print(ci_low.flatten().tolist()) \n",
    "\n",
    "# # TODO: validate these findings with the very basic network to see if it predicts that these stats will land us in our desired class\n",
    "# np.set_printoptions(suppress=True, precision=4)\n",
    "# print(confidence_interval_numpy(predicted_unscaled_vals, actual_unscaled_vals)) # requires the assumption of gaussianity, but seems to be working?\n",
    "\n",
    "\n",
    "# print(bootstrap_confidence_interval(predicted_unscaled_vals, actual_unscaled_vals)) # should be able to work without the assumption of gaussianity\n",
    "# # computing for rmse, so says that the true root mean squared error has a 95% of landing in these intervals\n",
    "# # residuals are likely far smaller because you have the negatives balancing things out so it is going to be less accurate for computing the confidence interval\n",
    "\n",
    "# residuals = predicted_unscaled_vals - actual_unscaled_vals\n",
    "# ci = np.quantile(residuals, 1 - 0.05, axis=0)\n",
    "# print(\"HELLO\")\n",
    "# print(ci)\n",
    "\n",
    "# # I want to say that my prediction is between these 2 bounds with 95% confidence\n",
    "# # The two functions I provided seem to find the rmse of each parameter with 95% confidence\n",
    "# # the first and 4th method I try do not scale the mean of the data and therefore, I don't think they necessarily apply to giving me the confidence interval I desire\n",
    "\n",
    "\n",
    "\n",
    "# # DO A CASE STUDY on an individual\n",
    "# # ANALYZE THIS MODEL ON BASELINE AND EXTERNAL\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.ensemble import RandomForestRegressor \n",
    "# random_forest_model = RandomForestRegressor(n_estimators = 60, random_state = 0)\n",
    "\n",
    "# random_forest_model.fit(X_train, y_train) \n",
    "\n",
    "# y_pred = random_forest_model.predict(X_test)\n",
    "\n",
    "# mse = mean_squared_error(y_test, y_pred)\n",
    "# rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "# print(\"MSE: \",mse)\n",
    "# print(\"RMSE: \",rmse)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
